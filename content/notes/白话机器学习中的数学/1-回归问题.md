## 2. 回归

### 2.3 最小二乘法

#### 目标函数（误差函数）
$$

\displaystyle E(\theta) = \frac{1}{2} \sum_{i=1}^n(y^{(i)}-f_{\theta}(x^{(i)}))^2

$$
找到使$E(\theta)$最小的$\theta$值的问题称为最优化问题。

#### 梯度下降法
$$

\large

\normalsize x:=x-\eta \frac{\normalsize d}{ \normalsize dx}g(x) 

$$
$\eta$称为学习率。

求损失函数$E(\theta)$对$\theta_1$和$\theta_2$的偏微分
$$

\large

\theta_0:=\theta_0-\eta\frac{\normalsize \partial E}{\normalsize \partial \theta_0} = \theta_0:=\theta_0 - \eta \sum_{i=1}^n(f_\theta(x^{(i)}-y^{(i)})

$$
$$

\large

\theta_1:=\theta_1-\eta\frac{\normalsize \partial E}{\normalsize \partial \theta_1}=\theta_1:=\theta_1 - \eta \sum_{i=1}^n(f_\theta(x^{(i)}-y^{(i)}) {x^{(i)}}

$$
令$u = E(\theta),v = f_{\theta}(x)$，则
$$

\large 

\frac{\normalsize \partial E}{\normalsize \partial \theta_0} = \frac{\normalsize \partial u}{\normalsize \partial v} \cdot \frac{\normalsize \partial v}{\normalsize \partial \theta_0} 

$$
### 2.4 多项式回归
$$

\large 

f_{\theta}(x) = \theta_0+\theta_1x+\theta_2x^2+...+\theta_nx^n

$$
$$

\large

\theta_2:=\theta_2 - \eta \sum_{i=1}^n(f_\theta(x^{(i)}-y^{(i)}) {x^{(i)}}^2

$$
### 2.5 多重回归

多个变量的函数可以写为$f_{\theta}(x_1,x_2,...,x_n) = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n$，用向量的形式可以写为：
$$

\large
f_{\theta}(\boldsymbol x)=\boldsymbol \theta^T \boldsymbol x
$$
其中$\boldsymbol \theta = \begin{bmatrix} \theta_0 & \theta_1 & \theta_2 &...& \theta_n \end{bmatrix}^T,\boldsymbol x = \begin{bmatrix} x_0 & x_1 & x_2 &... & x_n \end{bmatrix}^T,x_0=1$。

最速下降法
$$

\large

\theta_j:=\theta_j - \eta \sum_{i=1}^n(f_\theta(x^{(i)})-y^{(i)}) x_j^{(i)}

$$

### 2.6 随机梯度下降
随机梯度下降
$$

\large

\theta_j:=\theta_j - \eta (f_{\boldsymbol \theta}(\boldsymbol x^{(k)})-y^{(k)}) x_j^{(k)}

$$
小批量梯度下降(mini-batch)
$$

\large
\theta_j:=\theta_j - \eta \sum_{k \in K}(f_\theta(x^{(k)})-y^{(k)}) x_j^{(k)}
$$