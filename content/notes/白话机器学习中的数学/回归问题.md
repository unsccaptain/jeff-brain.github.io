## 2. 回归

### 2.3 最小二乘法

#### 目标函数（误差函数）
$$
\displaystyle E(\theta) = \frac{1}{2} \sum_{i=1}^n(y^{(i)}-f_{\theta}(x^{(i)}))^2
$$
找到使$E(\theta)$最小的$\theta$值的问题称为最优化问题。

#### 梯度下降法
$$
\normalsize x:=x-\eta \frac{\normalsize d}{ \normalsize dx}g(x) 
$$
$\eta$称为学习率。

求损失函数$E(\theta)$对$\theta_1$和$\theta_2$的偏微分
$$
\theta_0:=\theta_0-\eta\frac{\normalsize \partial E}{\normalsize \partial \theta_0} \\
\theta_1:=\theta_1-\eta\frac{\normalsize \partial E}{\normalsize \partial \theta_1}
$$
令$u = E(\theta),v = f_{\theta}(x)$，则
$$
\frac{\normalsize \partial E}{\normalsize \partial \theta_0} = \frac{\normalsize \partial u}{\normalsize \partial v} \cdot \frac{\normalsize \partial v}{\normalsize \partial \theta_0} 
$$
### 2.4 多项式回归
$$
 f_{\theta}(x) = \theta_0+\theta_1x+\theta_2x^2+...+\theta_nx^n
$$
多重回归

多个变量的函数可以写为$f_{\theta}(x_1,x_2,...,x_n) = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n$，如果用向量的形式可以写为：
$$
f_{\theta}(\boldsymbol x)=\boldsymbol \theta^T \boldsymbol x
$$
其中$\boldsymbol \theta = \begin{bmatrix} \theta_0 & \theta_1 & \theta_2 &...& \theta_n \end{bmatrix}^T,\boldsymbol x = \begin{bmatrix} x_0 & x_1 & x_2 &... & x_n \end{bmatrix}^T,x_0=1$。

随机梯度下降