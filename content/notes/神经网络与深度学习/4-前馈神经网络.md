4.1 神经元

一个神经元接受D个输入x_1,x_2,...x_D，令向量$x=[x_1;x_2;...;x_D]$来表示输入，并用净输入$z \in \mathbb{R}$表示一个神经元输入信号$\boldsymbol x$的加权和。
$$
\large
\begin{equation}
\begin{split}
z &= \sum_{d=1}^Dw_dx_d+b \\
&= \boldsymbol w^T \boldsymbol x + b
\end{split}
\end{equation}
$$
净输入$z$在经过一个非线性函数$f(\cdot)$后得到神经元的活性值，其中非线性函数$f(\cdot)$称为激活函数。

![[neuron.jpg]]

4.1.1 Sigmoid型函数

Sigmoid型函数指一类两端饱和的S型曲线函数。

Logistic函数
$$
\large
\sigma(x) = \frac{1}{1+exp(-x)}
$$
Tanh函数
$$
\large
tanh(x)=\frac{exp(x)-exp(-x)}{exp(x)+exp(-x)} = 2 \sigma(2x)-1
$$
Tanh以0为中心，而Logistic不以0为中心。

Hard-Logistic和Hard-Tanh

通过分段函数来近似Logistic函数和Tanh函数。

$$
\large
\begin{equation}
hard-logistic(x)=
\begin{cases} 
1 && g_l(x) \ge 1 \\
g_l && 0<g_l(x)<1 \\
0 && g_l(x) \le 0
\end{cases} 
\end{equation} 
$$

4.2 网络结构

4.2.1 前馈网络

整个网络中的信息是朝着一个方向传播的，没有反向的信息传播。前馈网络包括全连接前馈网络和卷积神经网络。

4.2.2 记忆网络

网络中的神经元不但可以接受其他神经元的信息，也可以接受自己的历史信息。其中的神经元具有记忆功能，在不同的时候有不同的状态。

记忆神经网络包括
+ 循环神经网络
+ Hopfield网络
+ 玻尔兹曼机
+ 受限玻尔兹曼机
+ ...

4.2.3 图网络

图网络是定义在图数据结构上的神经网络，图中每个节点都由一个或一组神经元构成。节点间的连接可以是有向的也可以是无向的。
![[net-structure.jpg]]
4.3 前馈神经网络

前馈神经网络（FNN）也称为多层感知器，通常由多层Logistic回归模型组成。

第0层称为输入层，最后一层称为输出层，其他中间层称为隐藏层。网络无反馈，信号从输入层向输出层单向传播，可用有向无环图表示。
![[FNN.jpg]]

4.3.1 通用近似定理

常见的连续非线性函数都可以用前馈神经网络来近似。

根据通用近似定理， 对于具有线性输出层和至少一个使用“挤压” 性质的激活函数的隐藏层组成的前馈神经网络， 只要其隐藏层神经元的数量足够， 它可以以任意的精度来近似任何一个定义在实数空间 ℝ𝐷 中的有界闭集函数。

4.3.2 应用到机器学习

4.4 反向传播算法

